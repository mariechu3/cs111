NAME: Marie Chu
UID: 905116878
EMAIL: mariechu@ucla.edu
SLIPDAYS: 0

Description of included contents of tar file:
(1) README- decript of contents of tar file and answers to questions
(2) Makefile- builds the executables of lab2_list and lab2_add as well as
generates the csv files, png files, and tar file
(3) lab2_add-1-5.pngs- generated graphs for the lab2_add
(4) lab2_list-1-4.pngs- generated graphs for the lab2_list
(5) lab2_list.csv- input file for gnuplot of lab2_list
(6) lab2_add.csv- input file for gnuplot of lab2_add
(7) lab2_list.c- source file to handle sync, yield, thread, and iteration options
(8) lab2_add.c- source file to handle sync, yield, thread, and iteration options
(9) lab2_add.gp- script to generate graphs for lab2_add
(10)lab2_list.gp- script to generate graphs for lab2_list
(11)tests.sh- test cases to generate lines for csv files
(12)SortedList.h- header file with functions declarations for SortedList.c
(13)SortedList.c- source file to handle insert, delete, lookup, and length 
for a shared circular doubly linked list.

QUESTION 2.1.1 -causing conflicts:
Why does it take many iterations before errors are seen?
    It takes so many iterations before errors are seen because the effects of race 
    conditions only happen when the timing of thread accesses is "right". By increasing
    the number of iterations, this increases the chances of threads accesssing/
    modifying at the "right" moment to cause the errors resulting from race
    conditions.
Why does a significantly smaller number of iterations so seldom fail?
    A significantly smaller number of iterations so seldomly fail because
    there are significantly fewer chances for them to acesses/modify at the
    "right" time to cause race conditions.

QUESTION 2.1.2 -cost of yielding:
Why are the --yield runs so much slower?
    The yield runs are a lot slower because at that point the thread gives up
    the CPU and returns control to the OS which decides the next thread to run.
    This adds additional overhead through context switches/additional switching
    between kernel/user modes.
Where is the additional time going?
    The additional time is going to the scheduling of the next thread and possible
    context switches in which registers need to be save and restored. 
Is it possible to get valid per-operation timings is we are using the --yield 
option? If so, explain how. If not, explain why not.
    It is not possible to get valid per-operation timings with the yield option
    because sched_yield() will relinquish the CPU and since it is called in the
    add function (in the middle of one operation) we will be unable to calculate
    valid timings that do not include the time it takes to relinquish the CPU and 
    regain control of the CPU. If we change our implementationi, however, to calculate the
    total time spent before and after the yield option and add the times up for every
    single thread, then it will be possible to get valid per-operation timings with
    --yield.

QUESTION 2.1.3-measurement errors:
Why does the average cost per operation drop with increasing iterations?
    The average cost of operations drops with increasing iterations, because of the
    way that we measure the total time of operations. We get the time before creating
    the threads and get the time after they join. This is not reflective of the actual
    time spent per operation because it is adding the additional overhead of thread
    creation and joining. Thus for less iterations, this overhead has more "effect" on
    the total time, but as the number of iterations increases, since we are only
    creating the thread once per thread it's effect to the overall time lessens and
    thus the average cost per operation also drops.    
If the cost per iteration is a function of the number of ierations, how do we
know how many iterations to run (or what the "correct" cost is)?
    With the current way of timing, it would be difficult to determine what the correct    
    cost per operation is. Obviously with the more number of iterations, the closer the
    value calculated should be to the average cost as the thread creation overhead
    becomes negligible. A better approach may be to get the starting time in each
    thread and ending time in each thread and having them report their values back to 
    the main thread when they join in order to calculate the average time of the
    operations themselves, this would take away the overhead of thread creation/joining.

QUESTION 2.1.4- costs of serialization:
Why do all of the options perform similarly for low numbers of threads?
    All the options perform similarly for low number threads because there is less 
    "waiting" for a thread to release the lock for another to obtain it. For example,
    in the case of one thread, all the values are very similar with minor discrepancy
    perhaps due to other processes running on the server or additional overhead from
    calling the lock/unlock mechanisms. Other than that, since there is only one thread
    running it would never have to wait for any other thread to unlock the lock as it is
    the only one that is running so it is the only one that could acquire a lock. 
Why do all three protected operations slow down as the number of threads rise?
    As the number of threads increases, the protected operations begin to slow down 
    because now one thread has to wait for another to unlock the lock before it can 
    acquire it and run. There is effectively close to no parallelism as the threads 
    perform the add functions sequentially because they are locked. Since mutex locks 
    make the thread trying to acquire a lock go to sleep instead of polling like spin 
    locks and wasting CPU, the cost per operations between the two are made apparent 
    as there are more threads and thus more waiting.

QUESTION 2.2.1--scalability of Mutex
Compare the variation in time per mutex-protected operations vs the number of threads in
Part-1(adds) and Part-2 (sorted lists). Comment on the general shapes of the curves, 
and explain why they have this shape.
    In both (add) and (sorted lists) the general trend is that the time per mutex-operation
    increases when the number of threads increase. The (add) graph looks like a flatter
    parabola, while the (sorted lists) graph looks like a straight line. They have this
    shape because the more threads that are added, the longer other threads have to wait
    to acquire the single lock that is used by all the threads. For example, if I only 
    have two threads, T2 at most has to wait for T1 to unlock the lock. However, if I
    have 32 threads. T32 may have to wait for T1 to lock and unlock, T2 to lock and unlock
    ... which would make the time for operations a lot longer for T32 and contribute a lot
    to the average cost of operations.  
Comment on the relative rates of increase and differences in the shapes of the curves, and
offer an explanation for these differences.
    The cost per operation seems to increase more steadily for (sorted lists) than (add). 
    This could be due to the fact that the critical sections for Sorted list are a lot 
    larger. Since the mutex locks are only locked for a short amount of time in (add), 
    especially when there are less threads, there is not much benefit or may even be 
    unbeneficial after accounting for the overhead of sleeping and waking up a thread. 
    Thus the slope of the graph is steeper in the beginning for add, and flattens out as 
    the overhead of sleeping/waking up is less effective as the waiting time increases.
    In lists, the critical sections are larger so there is not much variation in the slope 
    of the line as it is likely not unbeneficial to wake/sleep the thread.

QUESTION 2.2.2 - scalability of spin locks
Compare the variation in time per protected operation vs the number of threads for list 
operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves,
and explain why they have this shape.
    In the list operations, mutex and spin locks both have a general upward increase of 
    operation times that are almost linear. They have this shape because as the number of 
    threads increase, the longer one thread has to wait for another thread (and however many
    threads are in front of it trying to acquire the lock) to finish.
Comment on the relative rates of increase and differences in the shapes of the curves,
and offer an explanation for these differences.
    Both increase with approximately the same slope. However, with higher numbers of threads
    spin locks tend to become worse because with the longer time it takes to wait, the
    overhead of sleeping/waking up for mutexes is better than the fact that spin locks
    chew up CPU time doing useless work. Thus initially the cost for mutex locks may have
    been greater, but eventually, with more threads, spinlocks performed worse.
